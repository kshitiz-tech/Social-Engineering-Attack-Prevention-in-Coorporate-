{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cbf960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emails.csv...\n",
      "✅ Loaded and cleaned 617724 email messages.\n",
      "Processing row 0 at 2025-10-10 15:46:32.346222...\n",
      "Processing row 10000 at 2025-10-10 15:46:32.467526...\n",
      "Processing row 20000 at 2025-10-10 15:46:32.566483...\n",
      "Processing row 30000 at 2025-10-10 15:46:32.676205...\n",
      "Processing row 40000 at 2025-10-10 15:46:32.787565...\n",
      "Processing row 50000 at 2025-10-10 15:46:32.866333...\n",
      "Processing row 60000 at 2025-10-10 15:46:32.926701...\n",
      "Processing row 70000 at 2025-10-10 15:46:32.987511...\n",
      "Processing row 80000 at 2025-10-10 15:46:33.078712...\n",
      "Processing row 90000 at 2025-10-10 15:46:33.167462...\n",
      "Processing row 100000 at 2025-10-10 15:46:33.534846...\n",
      "Processing row 110000 at 2025-10-10 15:46:33.632005...\n",
      "Processing row 120000 at 2025-10-10 15:46:33.736597...\n",
      "Processing row 130000 at 2025-10-10 15:46:33.883837...\n",
      "Processing row 140000 at 2025-10-10 15:46:33.984787...\n",
      "Processing row 150000 at 2025-10-10 15:46:34.053941...\n",
      "Processing row 160000 at 2025-10-10 15:46:34.127214...\n",
      "Processing row 170000 at 2025-10-10 15:46:34.252130...\n",
      "Processing row 180000 at 2025-10-10 15:46:34.320743...\n",
      "Processing row 190000 at 2025-10-10 15:46:34.389976...\n",
      "Processing row 200000 at 2025-10-10 15:46:34.482879...\n",
      "Processing row 210000 at 2025-10-10 15:46:34.615378...\n",
      "Processing row 220000 at 2025-10-10 15:46:34.730155...\n",
      "Processing row 230000 at 2025-10-10 15:46:34.797920...\n",
      "Processing row 240000 at 2025-10-10 15:46:34.866353...\n",
      "Processing row 250000 at 2025-10-10 15:46:34.934199...\n",
      "Processing row 260000 at 2025-10-10 15:46:35.012793...\n",
      "Processing row 270000 at 2025-10-10 15:46:35.080984...\n",
      "Processing row 280000 at 2025-10-10 15:46:35.154982...\n",
      "Processing row 290000 at 2025-10-10 15:46:35.234226...\n",
      "Processing row 300000 at 2025-10-10 15:46:35.320472...\n",
      "Processing row 310000 at 2025-10-10 15:46:35.392837...\n",
      "Processing row 320000 at 2025-10-10 15:46:35.498812...\n",
      "Processing row 330000 at 2025-10-10 15:46:35.639422...\n",
      "Processing row 340000 at 2025-10-10 15:46:35.776043...\n",
      "Processing row 350000 at 2025-10-10 15:46:35.917409...\n",
      "Processing row 360000 at 2025-10-10 15:46:36.027340...\n",
      "Processing row 370000 at 2025-10-10 15:46:36.142529...\n",
      "Processing row 380000 at 2025-10-10 15:46:36.259653...\n",
      "Processing row 390000 at 2025-10-10 15:46:36.342345...\n",
      "Processing row 400000 at 2025-10-10 15:46:36.414989...\n",
      "Processing row 410000 at 2025-10-10 15:46:36.553456...\n",
      "Processing row 420000 at 2025-10-10 15:46:36.642259...\n",
      "Processing row 430000 at 2025-10-10 15:46:36.725577...\n",
      "Processing row 440000 at 2025-10-10 15:46:36.858249...\n",
      "Processing row 450000 at 2025-10-10 15:46:36.995970...\n",
      "Processing row 460000 at 2025-10-10 15:46:37.129058...\n",
      "Processing row 470000 at 2025-10-10 15:46:37.256061...\n",
      "Processing row 480000 at 2025-10-10 15:46:37.329877...\n",
      "Processing row 490000 at 2025-10-10 15:46:37.408342...\n",
      "Processing row 500000 at 2025-10-10 15:46:37.530447...\n",
      "Processing row 510000 at 2025-10-10 15:46:37.665299...\n",
      "Processing row 520000 at 2025-10-10 15:46:37.797939...\n",
      "Processing row 530000 at 2025-10-10 15:46:37.882709...\n",
      "Processing row 540000 at 2025-10-10 15:46:37.950884...\n",
      "Processing row 550000 at 2025-10-10 15:46:38.036603...\n",
      "Processing row 560000 at 2025-10-10 15:46:38.153604...\n",
      "Processing row 570000 at 2025-10-10 15:46:38.630292...\n",
      "Processing row 580000 at 2025-10-10 15:46:38.770109...\n",
      "Processing row 590000 at 2025-10-10 15:46:38.908425...\n",
      "Processing row 600000 at 2025-10-10 15:46:39.051588...\n",
      "Processing row 610000 at 2025-10-10 15:46:39.190765...\n",
      "Converting Dates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78950/3449478456.py:60: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  result_date = pd.to_datetime(result_date, errors='coerce', utc=True)\n",
      "/tmp/ipykernel_78950/3449478456.py:65: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<TimedeltaArray>\n",
      "['864 days 23:39:00', '854 days 20:51:00', '656 days 10:00:00',\n",
      " '661 days 13:13:00', '608 days 12:07:00', '608 days 11:17:00',\n",
      " '599 days 14:44:00', '560 days 13:59:00', '655 days 09:26:00',\n",
      " '654 days 13:44:00',\n",
      " ...\n",
      " '656 days 11:12:00', '657 days 20:25:00', '657 days 20:30:00',\n",
      " '620 days 15:44:00', '658 days 12:14:00', '661 days 10:28:00',\n",
      " '662 days 10:55:00', '663 days 09:21:00', '663 days 14:09:00',\n",
      " '663 days 23:55:00']\n",
      "Length: 617724, dtype: timedelta64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "  result_date[valid_mask] = result_date_valid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parsed messages: 617724\n",
      "After filtering valid entries: 281416\n",
      "✅ Graph built with 32609 nodes and 79118 edges.\n",
      "Mapped 32609 email addresses to integer IDs.\n",
      "✅ edges_with_daily_counts.csv saved successfully!\n",
      "   From  To  Count                                        DailyCounts\n",
      "0     0   1     19  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1     0   2     66  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2     0   3      2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3     0   4     14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4     0   5      4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Social Engineering Attack Prevention (Corporate Email Graph)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 1: Extract date, sender, recipient information\n",
    "# -----------------------------------------------------------\n",
    "def get_date_from_to(Series):\n",
    "    result_date = pd.Series(index=Series.index, dtype='object')\n",
    "    result_from = pd.Series(index=Series.index, dtype='object')\n",
    "    result_to = pd.Series(index=Series.index, dtype='object')\n",
    "\n",
    "    for row, message in enumerate(Series):\n",
    "        if row % 10000 == 0:\n",
    "            print(f'Processing row {row} at {datetime.now()}...')\n",
    "\n",
    "        if not isinstance(message, str):\n",
    "            result_date[row] = np.nan\n",
    "            result_from[row] = np.nan\n",
    "            result_to[row] = np.nan\n",
    "            continue\n",
    "\n",
    "        message_words = message.split('\\n')\n",
    "\n",
    "        # Extract Date\n",
    "        date_line = next((line for line in message_words if line.startswith('Date:')), None)\n",
    "        if date_line:\n",
    "            result_date[row] = date_line.replace('Date:', '').strip()\n",
    "        else:\n",
    "            result_date[row] = np.nan\n",
    "\n",
    "        # Extract From\n",
    "        from_line = next((line for line in message_words if line.startswith('From:')), None)\n",
    "        if from_line:\n",
    "            result_from[row] = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', from_line)\n",
    "        else:\n",
    "            result_from[row] = np.nan\n",
    "\n",
    "        # Extract To\n",
    "        to_line = next((line for line in message_words if line.startswith('To:')), None)\n",
    "        if to_line:\n",
    "            result_to[row] = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', to_line)\n",
    "        else:\n",
    "            result_to[row] = np.nan\n",
    "\n",
    "    # Convert to datetime (safe & consistent)\n",
    "    print('Converting Dates...')\n",
    "    result_date = pd.to_datetime(result_date, errors='coerce', utc=True)\n",
    "\n",
    "    # Compute timedelta since 1999-01-01\n",
    "    valid_mask = result_date.notna()\n",
    "    result_date_valid = result_date[valid_mask] - pd.Timestamp('1999-01-01', tz='UTC')\n",
    "    result_date[valid_mask] = result_date_valid\n",
    "\n",
    "    return result_date, result_from, result_to\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 2: Generate a social graph with daily email counts\n",
    "# -----------------------------------------------------------\n",
    "def gen_graph_with_daily_counts(date_from_to, total_days=1448):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for index, row in date_from_to.iterrows():\n",
    "        if pd.isna(row.date) or not isinstance(row.senders, list) or not isinstance(row.recipients, list):\n",
    "            continue\n",
    "\n",
    "        if isinstance(row.date, pd.Timedelta):\n",
    "            day = row.date.days\n",
    "            if not (0 <= day <= total_days):\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for sender in row.senders:\n",
    "            for recipient in row.recipients:\n",
    "                if sender == recipient:\n",
    "                    continue  # Skip self-emails\n",
    "\n",
    "                # Initialize edge if missing\n",
    "                if not G.has_edge(sender, recipient):\n",
    "                    G.add_edge(sender, recipient,\n",
    "                               count=0,\n",
    "                               daily_counts=np.zeros(total_days + 1, dtype=int))\n",
    "\n",
    "                # Update counts\n",
    "                G[sender][recipient]['count'] += 1\n",
    "                G[sender][recipient]['daily_counts'][day] += 1\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 3: Map nodes to integers\n",
    "# -----------------------------------------------------------\n",
    "def map_nodes_to_int(G):\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    H = nx.relabel_nodes(G, mapping)\n",
    "    return H, mapping\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 4: Load email data safely\n",
    "# -----------------------------------------------------------\n",
    "print(\"Loading emails.csv...\")\n",
    "emails = pd.read_csv('emails.csv', on_bad_lines='skip', engine='python')\n",
    "\n",
    "emails.columns = [c.strip().lower() for c in emails.columns]\n",
    "if 'file' not in emails.columns or 'message' not in emails.columns:\n",
    "    raise ValueError(\"❌ CSV must contain 'file' and 'message' columns!\")\n",
    "\n",
    "emails['file'] = emails['file'].astype(str)\n",
    "emails_nodups = emails[~emails['file'].str.contains('discussion_thread', na=False)]\n",
    "emails_noautos = emails_nodups[~emails_nodups['file'].str.contains('all_documents', na=False)]\n",
    "emails_noautos = emails_noautos.reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Loaded and cleaned {len(emails_noautos)} email messages.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 5: Extract structured date/from/to data\n",
    "# -----------------------------------------------------------\n",
    "date_from_to = pd.DataFrame()\n",
    "date_from_to['date'], date_from_to['senders'], date_from_to['recipients'] = get_date_from_to(emails_noautos['message'])\n",
    "\n",
    "print(\"Initial parsed messages:\", len(date_from_to))\n",
    "\n",
    "date_from_to.dropna(subset=['date', 'senders', 'recipients'], inplace=True)\n",
    "date_from_to = date_from_to[date_from_to.date >= pd.Timedelta(0)]\n",
    "date_from_to = date_from_to[date_from_to.date <= pd.Timedelta(days=1448)]\n",
    "print(\"After filtering valid entries:\", len(date_from_to))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 6: Build graph with daily counts\n",
    "# -----------------------------------------------------------\n",
    "G_init = gen_graph_with_daily_counts(date_from_to, total_days=1448)\n",
    "print(f\"✅ Graph built with {G_init.number_of_nodes()} nodes and {G_init.number_of_edges()} edges.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 7: Map nodes to integers\n",
    "# -----------------------------------------------------------\n",
    "H, mapping = map_nodes_to_int(G_init)\n",
    "print(f\"Mapped {len(mapping)} email addresses to integer IDs.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 8: Export edge data with daily arrays\n",
    "# -----------------------------------------------------------\n",
    "edges_data = []\n",
    "for u, v, data in H.edges(data=True):\n",
    "    edges_data.append({\n",
    "        'From': u,\n",
    "        'To': v,\n",
    "        'Count': int(data.get('count', 0)),\n",
    "        'DailyCounts': data['daily_counts'].tolist()  # convert numpy array to list for CSV\n",
    "    })\n",
    "\n",
    "edges_df = pd.DataFrame(edges_data)\n",
    "edges_df.to_csv('edges_with_daily_counts.csv', index=False)\n",
    "\n",
    "print(\"✅ edges_with_daily_counts.csv saved successfully!\")\n",
    "print(edges_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
