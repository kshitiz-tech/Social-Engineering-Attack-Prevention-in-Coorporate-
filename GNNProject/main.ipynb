{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840084c3-f771-42c9-be68-2b04e6acab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 6600\n",
      "Number of edges: 50897\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nodes_df = pd.read_csv('nodes.csv' , header = None , names=['node_attribute'], index_col=0)\n",
    "edges_df = pd.read_csv('edges.csv')\n",
    "\n",
    "\n",
    "nodes_list = [(index, ast.literal_eval(row.node_attribute))  for index, row in nodes_df.iterrows()]\n",
    "edges_list = [(row.From, row.To) for index, row in edges_df.iterrows()]\n",
    "\n",
    "number_of_days = 1448\n",
    "nodes_list_vec = []\n",
    "for index,dict_node in nodes_list:\n",
    "    vec = np.zeros(number_of_days,dtype=np.float32)\n",
    "    for day, count in dict_node.items():\n",
    "        vec[day] = count\n",
    "\n",
    "    nodes_list_vec.append((index, vec))\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "Graph = nx.DiGraph()\n",
    "for node_id, node_attr_vec in nodes_list_vec:\n",
    "    Graph.add_node(node_id, x = node_attr_vec)\n",
    "Graph.add_edges_from(edges_list)\n",
    "\n",
    "print(f\"Number of nodes: {nx.number_of_nodes(Graph)}\")\n",
    "print(f\"Number of edges: {nx.number_of_edges(Graph)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87609ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\libpyg.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_scatter\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_cluster\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_spline_conv\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_sparse\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\utils\\convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  data_dict[key] = torch.as_tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[6600, 1448], edge_index=[2, 50897])\n",
      "Data is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Convert to PyG data\n",
    "data = from_networkx(Graph)\n",
    "\n",
    "# Convert node attributes to tensor and move to device\n",
    "X = torch.stack([torch.tensor(attr['x'], dtype=torch.float32) for _, attr in Graph.nodes(data=True)])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = X.to(device)\n",
    "data.x = X\n",
    "data = data.to(device)\n",
    "\n",
    "print(data)\n",
    "print(f\"Data is on device: {data.x.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7be0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_score = []  #Holds every node's phase 1 score , index number corresponds to node id\n",
    "def phase1(node_list_vec, start_day = 0 , current_day=1448):\n",
    "    for node_id, node_attr_vec in node_list_vec:\n",
    "        mean = np.mean(node_attr_vec[start_day:current_day])\n",
    "        std = np.std(node_attr_vec[start_day:current_day])\n",
    "        today_score = (node_attr_vec[current_day] -mean) / std if std > 0 else 0\n",
    "        phase1_score.append(today_score)\n",
    "\n",
    "phase1(nodes_list_vec, start_day=0, current_day=1447)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7be0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "Epoch: 010, Loss: 3.1615\n",
      "Epoch: 020, Loss: 1.4410\n",
      "Epoch: 030, Loss: 1.4042\n",
      "Epoch: 040, Loss: 1.4076\n",
      "Epoch: 050, Loss: 1.3979\n",
      "Epoch: 060, Loss: 1.3922\n",
      "Epoch: 070, Loss: 1.3935\n",
      "Epoch: 080, Loss: 1.3909\n",
      "Epoch: 090, Loss: 1.3875\n",
      "Epoch: 100, Loss: 1.3856\n",
      "Epoch: 110, Loss: 1.3882\n",
      "Epoch: 120, Loss: 1.3826\n",
      "Epoch: 130, Loss: 1.3810\n",
      "Epoch: 140, Loss: 1.3799\n",
      "Epoch: 150, Loss: 1.3759\n",
      "Epoch: 160, Loss: 1.3734\n",
      "Epoch: 170, Loss: 1.3749\n",
      "Epoch: 180, Loss: 1.3723\n",
      "Epoch: 190, Loss: 1.3721\n",
      "Epoch: 200, Loss: 1.3649\n",
      "Epoch: 210, Loss: 1.3640\n",
      "Epoch: 220, Loss: 1.3609\n",
      "Epoch: 230, Loss: 1.3544\n",
      "Epoch: 240, Loss: 1.3462\n",
      "Epoch: 250, Loss: 1.3380\n",
      "Epoch: 260, Loss: 1.3287\n",
      "Epoch: 270, Loss: 1.3217\n",
      "Epoch: 280, Loss: 1.3113\n",
      "Epoch: 290, Loss: 1.2973\n",
      "Epoch: 300, Loss: 1.2824\n",
      "Epoch: 310, Loss: 1.2740\n",
      "Epoch: 320, Loss: 1.2717\n",
      "Epoch: 330, Loss: 1.2732\n",
      "Epoch: 340, Loss: 1.2637\n",
      "Epoch: 350, Loss: 1.2540\n",
      "Epoch: 360, Loss: 1.2588\n",
      "Epoch: 370, Loss: 1.2495\n",
      "Epoch: 380, Loss: 1.2530\n",
      "Epoch: 390, Loss: 1.2539\n",
      "Epoch: 400, Loss: 1.2523\n",
      "Epoch: 410, Loss: 1.2477\n",
      "Epoch: 420, Loss: 1.2415\n",
      "Epoch: 430, Loss: 1.2456\n",
      "Epoch: 440, Loss: 1.2342\n",
      "Epoch: 450, Loss: 1.2355\n",
      "Epoch: 460, Loss: 1.2316\n",
      "Epoch: 470, Loss: 1.2276\n",
      "Epoch: 480, Loss: 1.2282\n",
      "Epoch: 490, Loss: 1.2257\n",
      "\n",
      "Embeddings saved to 'final_embeddings.pt'\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "phase2_score = [] #Holds every node's phase 2 score , index number corresponds to node id\n",
    "\n",
    "for i in range(len(nodes_list_vec)):\n",
    "    phase2_score.append({})  \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels , num_layers=3 , dropout = 0.3):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p = self.dropout, training= self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x \n",
    "\n",
    "in_channels = data.num_node_features\n",
    "hidden_channels = 128 \n",
    "out_channels = 64 \n",
    "\n",
    "model = GraphSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(data.x, data.edge_index)\n",
    "\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edge_index.size(1),\n",
    "    ).to(device)  \n",
    "\n",
    "    pos_similarity = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=1)\n",
    "    pos_loss = F.logsigmoid(pos_similarity).mean()\n",
    "\n",
    "    neg_similarity = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=1)\n",
    "    neg_loss = F.logsigmoid(-neg_similarity).mean()\n",
    "\n",
    "    loss = -pos_loss - neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), z\n",
    "\n",
    "print(f\"Training on device: {next(model.parameters()).device}\")\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    loss, embeddings = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(data.x, data.edge_index)\n",
    "\n",
    "# saving file\n",
    "    torch.save(z, 'final_embeddings.pt')\n",
    "    print(\"\\nEmbeddings saved to 'final_embeddings.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de765a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{200: [(5508, 1)]}\n",
      "{200: [(2761, 0.15789473684210525), (5508, 0.8333333333333334)]}\n",
      "{200: [(679, 0.022727272727272728), (1069, 0.07142857142857142), (1460, 0.07142857142857142), (3042, 0.18421052631578946), (3454, 0.046511627906976744), (4043, 0.046511627906976744), (5355, 0.046511627906976744)]}\n",
      "{200: [(2275, 0.010416666666666666), (2525, 0.010416666666666666), (3401, 0.010416666666666666), (3798, 0.06593406593406594)]}\n",
      "{200: [(1327, 0.16666666666666666)]}\n",
      "{200: [(1789, 0.05660377358490566), (1828, 0.09803921568627451), (3223, 0.07692307692307693), (4072, 0.19148936170212766), (4834, 0.05660377358490566), (5581, 0.09803921568627451)]}\n",
      "{200: [(3442, 0.0625)]}\n",
      "{200: [(5053, 0.03180212014134275)]}\n",
      "{200: [(5508, 0.6666666666666666)]}\n",
      "{200: [(1327, 1)]}\n",
      "{200: [(1311, 0.05517241379310345), (5656, 0.05517241379310345)]}\n",
      "{200: [(4072, 0.05747126436781609), (4912, 0.013774104683195593), (6521, 0.027932960893854747)]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "def historical_pattern(current_day = 200):\n",
    "    csv_file = 'node_day_recipients.csv'\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[\"day_recipients_str\"] = df[\"day_recipients_str\"].apply(ast.literal_eval)\n",
    "\n",
    "    \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        node = row['node_id']\n",
    "        day_recipients = row['day_recipients_str']\n",
    "        if (current_day < len(day_recipients) and \n",
    "            len(day_recipients[current_day]) > 0):  \n",
    "            \n",
    "            recipients_today = day_recipients[current_day]\n",
    "            total_score_not_recipent = [0 for _ in range(len(recipients_today))]\n",
    "            total_score_recipent = [0 for _ in range(len(recipients_today))]\n",
    "\n",
    "            for past_day in range(current_day + 1):\n",
    "                if past_day < len(day_recipients) and len(day_recipients[past_day]) > 0:\n",
    "                    for past_recipient in day_recipients[past_day]:\n",
    "                        for j_index, current_recipient in enumerate(recipients_today):  \n",
    "                            if past_recipient == current_recipient:\n",
    "                                total_score_recipent[j_index] += 1\n",
    "                            else:\n",
    "                                total_score_not_recipent[j_index] += 1\n",
    "\n",
    "           \n",
    "            day_scores = []\n",
    "            for j_index, recipient in enumerate(recipients_today):\n",
    "                if total_score_not_recipent[j_index] == 0:\n",
    "                    score = 1\n",
    "                else:\n",
    "                    score = total_score_recipent[j_index] / total_score_not_recipent[j_index]\n",
    "                day_scores.append((recipient, score))\n",
    "            # Store in node_pattern_dict\n",
    "            phase2_score[node] = {current_day: day_scores}\n",
    "\n",
    "\n",
    "historical_pattern()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51355315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
