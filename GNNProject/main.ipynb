{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119ba2bf",
   "metadata": {},
   "source": [
    "# Social Engineering Analysis Pipeline\n",
    "\n",
    "This notebook organizes the full pipeline to compute Phase 1, Phase 2, Phase 3 scores and aggregate final scores for each node/day. It mirrors the implementation in `main.py` and exposes a class interface to run the pipeline for any day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5b5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Load nodes and edges\n",
    "nodes_df = pd.read_csv('nodes.csv' , header=None, names=['node_attribute'], index_col=0)\n",
    "edges_df = pd.read_csv('edges.csv')\n",
    "\n",
    "# Parse node attributes\n",
    "nodes_list = [(index, ast.literal_eval(row.node_attribute)) for index, row in nodes_df.iterrows()]\n",
    "edges_list = [(row.From, row.To) for index, row in edges_df.iterrows()]\n",
    "\n",
    "# Vectorize node attributes\n",
    "number_of_days = 1448\n",
    "nodes_list_vec = []\n",
    "for index, dict_node in nodes_list:\n",
    "    vec = np.zeros(number_of_days, dtype=np.float32)\n",
    "    for day, count in dict_node.items():\n",
    "        vec[day] = count\n",
    "    nodes_list_vec.append((index, vec))\n",
    "\n",
    "# Build graph\n",
    "Graph = nx.DiGraph()\n",
    "for node_id, node_attr_vec in nodes_list_vec:\n",
    "    Graph.add_node(node_id, x=node_attr_vec)\n",
    "Graph.add_edges_from(edges_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29714b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\libpyg.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_scatter\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_cluster\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_spline_conv\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\Lib\\site-packages\\torch_sparse\\_version_cuda.pyd\n",
      "  import torch_geometric.typing\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\barsa\\Documents\\Projects\\Social-Engineering-Attack-Prevention-in-Coorporate-\\GNNProject\\venv\\lib\\site-packages\\torch_geometric\\utils\\convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  data_dict[key] = torch.as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "# Device, PyG conversion, and embeddings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import from_networkx, negative_sampling\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Convert to PyG data\n",
    "data = from_networkx(Graph)\n",
    "\n",
    "# Node features to tensor\n",
    "X = torch.stack([torch.tensor(attr['x'], dtype=torch.float32) for _, attr in Graph.nodes(data=True)])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = X.to(device)\n",
    "data.x = X\n",
    "data = data.to(device)\n",
    "\n",
    "# Load or train embeddings once\n",
    "z = None\n",
    "embeddings_path = 'final_embeddings.pt'\n",
    "if os.path.exists(embeddings_path):\n",
    "    z = torch.load(embeddings_path, map_location=device)\n",
    "else:\n",
    "    class GraphSAGE(torch.nn.Module):\n",
    "        def __init__(self, in_channels, hidden_channels, out_channels , num_layers=3 , dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.convs = torch.nn.ModuleList([SAGEConv(in_channels, hidden_channels)])\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "            self.dropout = dropout\n",
    "        def forward(self, x, edge_index):\n",
    "            for conv in self.convs[:-1]:\n",
    "                x = conv(x, edge_index)\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.convs[-1](x, edge_index)\n",
    "            return x\n",
    "    in_channels = data.num_node_features\n",
    "    model = GraphSAGE(in_channels, 128, 64).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    def train_one():\n",
    "        model.train(); opt.zero_grad()\n",
    "        z_local = model(data.x, data.edge_index)\n",
    "        pos = data.edge_index\n",
    "        neg = negative_sampling(edge_index=data.edge_index, num_nodes=data.num_nodes, num_neg_samples=pos.size(1)).to(device)\n",
    "        pos_loss = F.logsigmoid((z_local[pos[0]]*z_local[pos[1]]).sum(dim=1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(z_local[neg[0]]*z_local[neg[1]]).sum(dim=1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "        loss.backward(); opt.step()\n",
    "        return z_local\n",
    "    for _ in range(500):\n",
    "        _ = train_one()\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.edge_index)\n",
    "    torch.save(z, embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e319087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions and globals\n",
    "\n",
    "def sigmoid_norm(x, alpha=0.5):\n",
    "    return float(1 / (1 + np.exp(-alpha * x)))\n",
    "\n",
    "phase1_score = []\n",
    "phase2_score = [0] * len(nodes_list)\n",
    "phase3_score = [0] * len(nodes_list)\n",
    "final_score = [[] for _ in range(len(nodes_list))]\n",
    "\n",
    "# Phase 1\n",
    "def phase1(node_list_vec, start_day=0, current_day=220):\n",
    "    for _, node_attr_vec in node_list_vec:\n",
    "        mean = np.mean(node_attr_vec[start_day:current_day])\n",
    "        std = np.std(node_attr_vec[start_day:current_day])\n",
    "        today_score = (node_attr_vec[current_day] - mean) / std if std > 0 else 0\n",
    "        phase1_score.append(sigmoid_norm(today_score, alpha=0.5))\n",
    "\n",
    "# Phase 2\n",
    "import ast as _ast  # avoid shadowing\n",
    "\n",
    "def historical_pattern(current_day=220, weight_distribution=0.3):\n",
    "    csv_file = 'node_day_recipients.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['day_recipients_str'] = df['day_recipients_str'].apply(_ast.literal_eval)\n",
    "    for _, row in df.iterrows():\n",
    "        node = row['node_id']\n",
    "        day_recipients = row['day_recipients_str']\n",
    "        if current_day < len(day_recipients) and len(day_recipients[current_day]) > 0:\n",
    "            recipients_today = day_recipients[current_day]\n",
    "            total_not = [0 for _ in range(len(recipients_today))]\n",
    "            total_yes = [0 for _ in range(len(recipients_today))]\n",
    "            for past_day in range(current_day + 1):\n",
    "                if past_day < len(day_recipients) and len(day_recipients[past_day]) > 0:\n",
    "                    for past_recipient in day_recipients[past_day]:\n",
    "                        for j_idx, current_recipient in enumerate(recipients_today):\n",
    "                            if past_recipient == current_recipient:\n",
    "                                total_yes[j_idx] += 1\n",
    "                            else:\n",
    "                                total_not[j_idx] += 1\n",
    "            day_scores = []\n",
    "            for j_idx, recipient in enumerate(recipients_today):\n",
    "                if total_not[j_idx] == 0:\n",
    "                    historical_score = 1.0\n",
    "                else:\n",
    "                    historical_score = total_yes[j_idx] / total_not[j_idx]\n",
    "                try:\n",
    "                    node_embedding = z[node]\n",
    "                    recipient_embedding = z[recipient]\n",
    "                    cos = F.cosine_similarity(node_embedding.unsqueeze(0), recipient_embedding.unsqueeze(0)).item()\n",
    "                    cos = (cos + 1) / 2\n",
    "                    raw_score = (weight_distribution * historical_score + (1 - weight_distribution) * (1 - cos))\n",
    "                except Exception:\n",
    "                    raw_score = historical_score\n",
    "                final_s = sigmoid_norm(raw_score, alpha=0.5)\n",
    "                day_scores.append((recipient, final_s))\n",
    "            phase2_score[node] = day_scores\n",
    "\n",
    "# Phase 3\n",
    "import community.community_louvain as community_louvain\n",
    "from collections import defaultdict\n",
    "\n",
    "def phase3_community_normalization(Graph):\n",
    "    undirected_graph = Graph.to_undirected()\n",
    "    partition = community_louvain.best_partition(undirected_graph)\n",
    "    all_fraction_same, senders = [], []\n",
    "    for sender, rec_scores in enumerate(phase2_score):\n",
    "        if not rec_scores:\n",
    "            continue\n",
    "        c_send = partition.get(sender, -1)\n",
    "        recipients = [r for r, _ in rec_scores]\n",
    "        if c_send == -1:\n",
    "            continue\n",
    "        same_comm = sum(1 for r in recipients if partition.get(r, -1) == c_send)\n",
    "        total = len(recipients)\n",
    "        frac = same_comm / total if total > 0 else 0\n",
    "        all_fraction_same.append(frac)\n",
    "        senders.append(sender)\n",
    "    mean_fraction = np.mean(all_fraction_same) if all_fraction_same else 0\n",
    "    for sender, frac in zip(senders, all_fraction_same):\n",
    "        phase3_score[sender] = float(1 / (1 + np.exp(0.5 * (frac - mean_fraction))))\n",
    "\n",
    "# Aggregation\n",
    "def aggregate_score(w2=0.6, w1=0.2, w3=0.2):\n",
    "    for node_id in range(len(nodes_list)):\n",
    "        entry = phase2_score[node_id]\n",
    "        if entry:\n",
    "            p1_score = phase1_score[node_id] if node_id < len(phase1_score) else 0.0\n",
    "            p3_score = phase3_score[node_id] if node_id < len(phase3_score) and phase3_score[node_id] > 0 else 1\n",
    "            for recipient, score in entry:\n",
    "                final_score_node = w2 * score + w1 * p1_score + w3 * p3_score\n",
    "                final_score[node_id].append((recipient, final_score_node))\n",
    "            if final_score[node_id]:\n",
    "                print(node_id, final_score[node_id])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a5ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [(5214, 0.5157558806897105)]\n",
      "449 [(3896, 0.535885576361866)]\n",
      "525 [(6147, 0.5643851122226402)]\n",
      "805 [(1870, 0.5290141139447467)]\n",
      "964 [(2553, 0.5177668203377438)]\n",
      "1247 [(2979, 0.562182229055529)]\n",
      "1588 [(5603, 0.49220492233523805), (5921, 0.4900605569620562)]\n",
      "1710 [(1367, 0.49600796781728845), (1925, 0.4959347157294957), (5300, 0.49580356416385407)]\n",
      "1824 [(2719, 0.5219916001512845)]\n",
      "2008 [(519, 0.6174972249068544), (1742, 0.6204888153456272), (2186, 0.6189367238762782), (2208, 0.6197959624593901), (2512, 0.6247131612957906), (3377, 0.6191782571470665), (4176, 0.6195915430401082), (4515, 0.6243570990287384), (5211, 0.6261173006109421), (6289, 0.6194641526835274)]\n",
      "2101 [(1870, 0.5291952755468599)]\n",
      "2213 [(1026, 0.5282463683746215), (3107, 0.5108828188685254)]\n",
      "2289 [(700, 0.5392220237811274), (2291, 0.5404091709670321), (4061, 0.5397759339108417), (5461, 0.541982619928871)]\n",
      "2368 [(617, 0.5049819161236213), (2979, 0.5057847442441566), (4758, 0.49965251956468), (5118, 0.4974690991575885), (5410, 0.49760211110415603), (6015, 0.4973073302844037), (6061, 0.4970806609446911)]\n",
      "2374 [(1870, 0.5287620372534574)]\n",
      "2471 [(5508, 0.653781445971418)]\n",
      "2553 [(964, 0.5719093904941498), (5316, 0.5716902440174272)]\n",
      "2828 [(2828, 0.5153723503698333)]\n",
      "2901 [(2186, 0.5640136584880625)]\n",
      "3039 [(3003, 0.4895409470555524)]\n",
      "3386 [(1934, 0.6017227240922071)]\n",
      "3496 [(446, 0.5936697924717365), (2588, 0.5927253495182584), (3686, 0.5924803728023743), (3851, 0.5940811990925248), (6210, 0.5938982057602631), (6289, 0.5924246480335783)]\n",
      "3619 [(520, 0.5045443628071384), (775, 0.504591956703228), (964, 0.5045443628071384), (1289, 0.5051568299181275), (2272, 0.5091465892933247), (2292, 0.5052216883004709), (2342, 0.5101057974399512), (2553, 0.5051008113466813), (3291, 0.5045443628071384), (3390, 0.5046378341820559), (3843, 0.5049658212979803), (4045, 0.5045617878823367), (4146, 0.5049527068436203), (4257, 0.5045443628071384), (6029, 0.5049527068436203), (6102, 0.5049527068436203)]\n",
      "4272 [(4176, 0.6943287420331655)]\n",
      "4543 [(512, 0.5728268417869056), (1754, 0.5727805085610902), (4052, 0.5733764055791254), (4605, 0.5726092878824007), (5337, 0.5754600219979387)]\n",
      "4689 [(3862, 0.5890428464421325)]\n",
      "4790 [(3759, 0.49716306739043536), (4275, 0.49269089916224296), (5214, 0.4968802661698554)]\n",
      "4896 [(552, 0.5124056187751784), (609, 0.5104377546512407), (1225, 0.5120993700357865), (1932, 0.5115330970650447), (2172, 0.5123866413935445), (3296, 0.5103765555536898), (3675, 0.5124131377549628), (4041, 0.512279036271754), (4896, 0.5122564181322906), (5445, 0.5122927226023883)]\n",
      "5022 [(6215, 0.5259426872060079)]\n",
      "5023 [(3170, 0.5179023651921775)]\n",
      "5214 [(6, 0.5091732495946852), (3759, 0.5095324444067055), (4275, 0.5091109646792699), (4790, 0.5093405643506713)]\n",
      "5276 [(2553, 0.6061052343180913)]\n",
      "5693 [(2553, 0.5052446701168686), (5276, 0.5052574685638627)]\n",
      "5696 [(3733, 0.5799912465348632)]\n",
      "6197 [(2052, 0.49338984259042534), (4382, 0.4952779501098166)]\n",
      "6370 [(3257, 0.5801149019221434)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5214, 0.5157558806897105)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3896, 0.535885576361866)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(6147, 0.5643851122226402)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(1870, 0.5290141139447467)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2553, 0.5177668203377438)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class wrapper\n",
    "class SEAMain:\n",
    "    def __init__(self, w2=0.6, w1=0.2, w3=0.2, hist_weight=0.3):\n",
    "        self.w2 = w2\n",
    "        self.w1 = w1\n",
    "        self.w3 = w3\n",
    "        self.hist_weight = hist_weight\n",
    "    def reset(self):\n",
    "        global phase1_score, phase2_score, phase3_score, final_score\n",
    "        phase1_score = []\n",
    "        phase2_score = [[] for _ in range(len(nodes_list))]\n",
    "        phase3_score = [0] * len(nodes_list)\n",
    "        final_score = [[] for _ in range(len(nodes_list))]\n",
    "    def phase1(self, day):\n",
    "        cd = max(0, min(int(day), number_of_days - 1))\n",
    "        phase1(nodes_list_vec, start_day=0, current_day=cd)\n",
    "    def phase2(self, day):\n",
    "        cd = max(0, min(int(day), number_of_days - 1))\n",
    "        historical_pattern(current_day=cd, weight_distribution=self.hist_weight)\n",
    "    def phase3(self):\n",
    "        phase3_community_normalization(Graph)\n",
    "    def aggregate(self):\n",
    "        aggregate_score(w2=self.w2, w1=self.w1, w3=self.w3)\n",
    "        return final_score\n",
    "        \n",
    "    def run(self, day):\n",
    "        self.reset()\n",
    "        self.phase1(day)\n",
    "        self.phase2(day)\n",
    "        self.phase3()\n",
    "        return self.aggregate()\n",
    "\n",
    "\n",
    "app = SEAMain()\n",
    "app.run(day=1024)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
