{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119ba2bf",
   "metadata": {},
   "source": [
    "# Social Engineering Analysis Pipeline\n",
    "\n",
    "This notebook organizes the full pipeline to compute Phase 1, Phase 2, Phase 3 scores and aggregate final scores for each node/day. It mirrors the implementation in `main.py` and exposes a class interface to run the pipeline for any day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f5b5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Load nodes and edges\n",
    "nodes_df = pd.read_csv('nodes.csv' , header=None, names=['node_attribute'], index_col=0)\n",
    "edges_df = pd.read_csv('edges.csv')\n",
    "\n",
    "# Parse node attributes\n",
    "nodes_list = [(index, ast.literal_eval(row.node_attribute)) for index, row in nodes_df.iterrows()]\n",
    "edges_list = [(row.From, row.To) for index, row in edges_df.iterrows()]\n",
    "\n",
    "# Vectorize node attributes\n",
    "number_of_days = 1448\n",
    "nodes_list_vec = []\n",
    "for index, dict_node in nodes_list:\n",
    "    vec = np.zeros(number_of_days, dtype=np.float32)\n",
    "    for day, count in dict_node.items():\n",
    "        vec[day] = count\n",
    "    nodes_list_vec.append((index, vec))\n",
    "\n",
    "# Build graph\n",
    "Graph = nx.DiGraph()\n",
    "for node_id, node_attr_vec in nodes_list_vec:\n",
    "    Graph.add_node(node_id, x=node_attr_vec)\n",
    "Graph.add_edges_from(edges_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29714b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device, PyG conversion, and embeddings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import from_networkx, negative_sampling\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Convert to PyG data\n",
    "data = from_networkx(Graph)\n",
    "\n",
    "# Node features to tensor\n",
    "X = torch.stack([torch.tensor(attr['x'], dtype=torch.float32) for _, attr in Graph.nodes(data=True)])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = X.to(device)\n",
    "data.x = X\n",
    "data = data.to(device)\n",
    "\n",
    "# Load or train embeddings once\n",
    "z = None\n",
    "model_path = \"graphsage_model.pt\"\n",
    "\n",
    "# ----- FIX #1: GraphSAGE must be defined before loading -----\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels , num_layers=3 , dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList([SAGEConv(in_channels, hidden_channels)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "in_channels = data.num_node_features\n",
    "model = GraphSAGE(in_channels, 128, 64).to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "else:\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    def train_one():\n",
    "        model.train(); opt.zero_grad()\n",
    "        z_local = model(data.x, data.edge_index)\n",
    "        pos = data.edge_index\n",
    "        neg = negative_sampling(\n",
    "            edge_index=data.edge_index, \n",
    "            num_nodes=data.num_nodes, \n",
    "            num_neg_samples=pos.size(1)\n",
    "        ).to(device)\n",
    "\n",
    "        pos_loss = F.logsigmoid((z_local[pos[0]] * z_local[pos[1]]).sum(dim=1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(z_local[neg[0]] * z_local[neg[1]]).sum(dim=1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "        loss.backward(); opt.step()\n",
    "        return z_local\n",
    "\n",
    "    for _ in range(500):\n",
    "        _ = train_one()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.edge_index)\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Generate embeddings for current graph\n",
    "with torch.no_grad():\n",
    "    z = model(data.x, data.edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e319087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nodes and edges\n",
    "nodes_df_synthetic = pd.read_csv('nodes-synthetic.csv' , header=None, names=['node_attribute'], index_col=0)\n",
    "edges_df_synthetic = pd.read_csv('edges-synthetic.csv')\n",
    "\n",
    "# Parse node attributes\n",
    "nodes_list_synthetic = [(index, ast.literal_eval(row.node_attribute)) for index, row in nodes_df_synthetic.iterrows()]\n",
    "edges_list_synthetic = [(row.From, row.To) for index, row in edges_df_synthetic.iterrows()]\n",
    "\n",
    "# Vectorize node attributes\n",
    "number_of_days = 1448\n",
    "nodes_list_vec_synthetic = []\n",
    "for index, dict_node in nodes_list_synthetic:\n",
    "    vec = np.zeros(number_of_days, dtype=np.float32)\n",
    "    for day, count in dict_node.items():\n",
    "        vec[day] = count\n",
    "    nodes_list_vec_synthetic.append((index, vec))\n",
    "\n",
    "#Create a new graph with new synthetic nodes to model the connection. No need to reporcess everything. will only add new nodes\n",
    "#This can be extracted programatically , but since this is for research purposes , we work with hardcoded ids because we know which are new nodes\n",
    "new_node_ids = [6600, 6601, 6602, 6603]\n",
    "# Add new nodes with features\n",
    "for nid, vec in nodes_list_vec_synthetic:\n",
    "    if nid not in Graph:       # Graph = original NetworkX graph\n",
    "        Graph.add_node(nid, x=vec)\n",
    "\n",
    "# Add edges (may include new edges)\n",
    "for u, v in edges_list_synthetic:\n",
    "    Graph.add_edge(u, v)\n",
    "\n",
    "data = from_networkx(Graph)\n",
    "\n",
    "X = torch.stack([\n",
    "    torch.tensor(attr['x'], dtype=torch.float32)\n",
    "    for _, attr in Graph.nodes(data=True)\n",
    "])\n",
    "\n",
    "data.x = X.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "model = GraphSAGE(in_channels, 128, 64).to(device)\n",
    "model.load_state_dict(torch.load(\"graphsage_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model(data.x, data.edge_index)\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions and globals\n",
    "\n",
    "def sigmoid_norm(x, alpha=0.5):\n",
    "    return float(1 / (1 + np.exp(-alpha * x)))\n",
    "\n",
    "phase1_score = []\n",
    "phase2_score = [0] * len(nodes_list_vec_synthetic)\n",
    "phase3_score = [0] * len(nodes_list_vec_synthetic)\n",
    "final_score = [[] for _ in range(len(nodes_list_vec_synthetic))]\n",
    "\n",
    "# Phase 1\n",
    "def phase1(node_list_vec_synthetic, start_day=0, current_day=211):\n",
    "    for _, node_attr_vec in node_list_vec_synthetic:\n",
    "        mean = np.mean(node_attr_vec[start_day:current_day])\n",
    "        std = np.std(node_attr_vec[start_day:current_day])\n",
    "        today_score = (node_attr_vec[current_day] - mean) / std if std > 0 else 0\n",
    "        phase1_score.append(sigmoid_norm(today_score, alpha=0.5))\n",
    "\n",
    "# Phase 2\n",
    "import ast as _ast  # avoid shadowing\n",
    "\n",
    "def historical_pattern(current_day, weight_distribution=0.3):\n",
    "    csv_file = 'node_day_recipients_synthetic.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['day_recipients_str'] = df['day_recipients_str'].apply(_ast.literal_eval)\n",
    "    for _, row in df.iterrows():\n",
    "        node = row['node_id']\n",
    "        day_recipients = row['day_recipients_str']\n",
    "        if current_day < len(day_recipients) and len(day_recipients[current_day]) > 0:\n",
    "            recipients_today = day_recipients[current_day]\n",
    "            total_not = [0 for _ in range(len(recipients_today))]\n",
    "            total_yes = [0 for _ in range(len(recipients_today))]\n",
    "            for past_day in range(current_day + 1):\n",
    "                if past_day < len(day_recipients) and len(day_recipients[past_day]) > 0:\n",
    "                    for past_recipient in day_recipients[past_day]:\n",
    "                        for j_idx, current_recipient in enumerate(recipients_today):\n",
    "                            if past_recipient == current_recipient:\n",
    "                                total_yes[j_idx] += 1\n",
    "                            else:\n",
    "                                total_not[j_idx] += 1\n",
    "            day_scores = []\n",
    "            for j_idx, recipient in enumerate(recipients_today):\n",
    "                if total_not[j_idx] == 0:\n",
    "                    historical_score = 1.0\n",
    "                else:\n",
    "                    historical_score = total_yes[j_idx] / total_not[j_idx]\n",
    "                try:\n",
    "                    node_embedding = z[node]\n",
    "                    recipient_embedding = z[recipient]\n",
    "                    cos = F.cosine_similarity(node_embedding.unsqueeze(0), recipient_embedding.unsqueeze(0)).item()\n",
    "                    cos = (cos + 1) / 2\n",
    "                    raw_score = (weight_distribution * historical_score + (1 - weight_distribution) * (1 - cos))\n",
    "                except Exception as e:\n",
    "                    print(\"EMBED ERROR:\", node, recipient, e)\n",
    "                    raw_score = historical_score\n",
    "                final_s = sigmoid_norm(raw_score, alpha=0.5)\n",
    "                day_scores.append((recipient, final_s))\n",
    "            phase2_score[node] = day_scores\n",
    "\n",
    "# Phase 3\n",
    "import community.community_louvain as community_louvain\n",
    "from collections import defaultdict\n",
    "\n",
    "def phase3_community_normalization(Graph):\n",
    "    undirected_graph = Graph.to_undirected()\n",
    "    partition = community_louvain.best_partition(undirected_graph)\n",
    "    all_fraction_same, senders = [], []\n",
    "    for sender, rec_scores in enumerate(phase2_score):\n",
    "        if not rec_scores:\n",
    "            continue\n",
    "        c_send = partition.get(sender, -1)\n",
    "        recipients = [r for r, _ in rec_scores]\n",
    "        if c_send == -1:\n",
    "            continue\n",
    "        same_comm = sum(1 for r in recipients if partition.get(r, -1) == c_send)\n",
    "        total = len(recipients)\n",
    "        frac = same_comm / total if total > 0 else 0\n",
    "        all_fraction_same.append(frac)\n",
    "        senders.append(sender)\n",
    "    mean_fraction = np.mean(all_fraction_same) if all_fraction_same else 0\n",
    "    for sender, frac in zip(senders, all_fraction_same):\n",
    "        phase3_score[sender] = float(1 / (1 + np.exp(0.5 * (frac - mean_fraction))))\n",
    "\n",
    "# Aggregation\n",
    "def aggregate_score(w2=0.6, w1=0.2, w3=0.2):\n",
    "    for node_id in range(len(nodes_list_synthetic)):\n",
    "        entry = phase2_score[node_id]\n",
    "        if entry:\n",
    "            p1_score = phase1_score[node_id] if node_id < len(phase1_score) else 0.0\n",
    "            p3_score = phase3_score[node_id] if node_id < len(phase3_score) and phase3_score[node_id] > 0 else 1\n",
    "            for recipient, score in entry:\n",
    "                final_score_node = w2 * score + w1 * p1_score + w3 * p3_score\n",
    "                final_score[node_id].append((recipient, final_score_node))\n",
    "                \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "98a5ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class wrapper\n",
    "class SEAMain:\n",
    "    def __init__(self, w2=0.6, w1=0.2, w3=0.2, hist_weight=0.3):\n",
    "        self.w2 = w2\n",
    "        self.w1 = w1\n",
    "        self.w3 = w3\n",
    "        self.hist_weight = hist_weight\n",
    "    def reset(self):\n",
    "        global phase1_score, phase2_score, phase3_score, final_score\n",
    "        phase1_score = []\n",
    "        phase2_score = [[] for _ in range(len(nodes_list_vec_synthetic))]\n",
    "        phase3_score = [0] * len(nodes_list_vec_synthetic)\n",
    "        final_score = [[] for _ in range(len(nodes_list_vec_synthetic))]\n",
    "    def phase1(self, day):\n",
    "        phase1(nodes_list_vec_synthetic, start_day=0, current_day= day)\n",
    "    def phase2(self, day):\n",
    "        historical_pattern(current_day=day, weight_distribution=self.hist_weight)\n",
    "    def phase3(self):\n",
    "        phase3_community_normalization(Graph)\n",
    "    def aggregate(self):\n",
    "        aggregate_score(w2=self.w2, w1=self.w1, w3=self.w3)\n",
    "        return final_score\n",
    "        \n",
    "    def run(self, day):\n",
    "        self.reset()\n",
    "        self.phase1(day)\n",
    "        self.phase2(day)\n",
    "        self.phase3()\n",
    "        return self.aggregate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7640c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20 Feb 1999'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_date = pd.Timestamp('1999-01-01', tz='UTC')\n",
    "\n",
    "def day_index_to_date(day_index: int, tz: str = 'UTC') -> pd.Timestamp:\n",
    "    di = int(day_index)\n",
    "    dt = base_date + pd.Timedelta(days=di)\n",
    "    return dt.tz_convert(tz) if tz and dt.tzinfo is not None and tz != 'UTC' else dt\n",
    "\n",
    "def format_human_date(ts) -> str:\n",
    "    \"\"\"Return dates like '4 May 2001' (abbreviated month, no leading zero).\"\"\"\n",
    "    ts = pd.Timestamp(ts)\n",
    "    for fmt in ('%-d %b %Y', '%#d %b %Y', '%d %b %Y'):\n",
    "        try:\n",
    "            s = ts.strftime(fmt)\n",
    "            if fmt == '%d %b %Y':\n",
    "                s = s.lstrip('0')\n",
    "            return s\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return ts.strftime('%Y-%m-%d')\n",
    "\n",
    "format_human_date(day_index_to_date(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9208fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching emails for date:  8 Apr 1999\n",
      "Count matches: 0\n"
     ]
    }
   ],
   "source": [
    "def show_emails_for_anomalies(anomalies, day_index):\n",
    "    \"\"\"Display email messages related to detected anomalies.\"\"\"\n",
    "    day_ts = day_index_to_date(day_index-3)\n",
    "    day_str = \" \" + format_human_date(day_ts)\n",
    "    print(f\"\\nSearching emails for date: {day_str}\")\n",
    "\n",
    "    emails = pd.read_csv(\"emails_synthetic.csv\", names=['file', 'message'], header=0)\n",
    "    id_email = pd.read_csv(\"id-email-synthetic.csv\")\n",
    "\n",
    "    count_matches = 0\n",
    "    for anomaly in anomalies:\n",
    "        sender_id, (recv_id, score) = list(anomaly.items())[0]\n",
    "        sender_email = id_email.iloc[sender_id-1][1]\n",
    "        receiver_email = id_email.iloc[recv_id-1][1]\n",
    "\n",
    "        print(f\"\\n[Anomaly] Node {sender_id} â†’ Node {recv_id} | Score={score:.3f}\")\n",
    "        print(f\"Sender: {sender_email}\")\n",
    "        print(f\"Receiver: {receiver_email}\")\n",
    "\n",
    "        matches = emails[\n",
    "            emails['message'].str.contains(day_str, na=False)\n",
    "            & emails['message'].str.contains(sender_email, na=False)\n",
    "            & emails['message'].str.contains(receiver_email, na=False)\n",
    "        ]\n",
    "        \n",
    "        if not matches.empty:\n",
    "            print(f\"\\nðŸ“§ Found {len(matches)} matching email(s):\\n\")\n",
    "            for i, row in matches.iterrows(): \n",
    "                print(row['message'], '...\\n')\n",
    "                count_matches += 1\n",
    "\n",
    "    print(f\"Count matches: {count_matches}\")\n",
    "\n",
    "show_emails_for_anomalies(anomalies, day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
